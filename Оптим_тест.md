[TOC]

# Introduction

The golden section method is considered to minimize the function of one variable.

The essence of this method is as follows. The uncertainty interval is divided into two unequal parts, so that the ratio of the length of the larger segment to the length of the entire interval is equal to the ratio of the length of the smaller segment to the length of the larger one (Fig. 1)

![image-20191213135133141](C:\Users\Astemir\Desktop\image-20191213135133141.png)

At each step of this iterative procedure, except for the first, only one function value is calculated. However, Himmelblau recommended calculating two points at each step so that the error does not accumulate, since τ (tau) has an approximate value (Fig. 2) (1)

# Algorithm

f (x): [a, b] → R, f (x) ∈C ([a, b]) in order to find the indefinite value of this function on a given interval that meets the search criterion (let it be a minimum), considered the segment is divided in proportion to the golden ratio in both directions, that is, two points x1 and x_2 are selected such that:
$$
\frac{b-a}{b-x_{1}} = \frac{b-a}{x_{2}-a}= φ = \frac{-1+√5}{2} =1.618...
$$
Thus:
$$
x_{1}=\frac{b-(b-a)}{φ}
$$

$$
x_{2}= \frac{a+(b-a)}{φ}
$$

That is, the point x1 divides the segment [a, x2] in relation to the golden ratio. Similarly, x2 divides the segment [x1, b] in the same proportion. This property is used to build an iterative process. (3)

1. At the first iteration, the given segment is divided into two points symmetric about its center, and the values at these points are calculated.
2. After that, one of the ends of the segment to which among the two newly defined points was closer is the one whose value is maximum (for the case of finding the minimum). (2)
3. At the next iteration, due to the golden ratio property shown above, it is already necessary to search for only one new point. (8)
4. The procedure continues until the desired accuracy is achieved. (2)

# Formalization

Step 1. The initial boundaries of the segment a, b and the accuracy ε are specified.

Step 2. Calculate the starting points of division:
$$
x_{1}=\frac{b-(b-a)}{φ}
$$

$$
x_{2}= \frac{a+(b-a)}{φ}
$$

and the values in them of the objective function:
$$
   y_{2}=f(x_{1}),y_{2}=f(x_{2}).
$$


   - If y1> y2, then a = x1
   - Otherwise b = x2

Step 3

- If | b-a | <ε, then
  $$
  x=\frac{a+b}{2}
  $$

- Otherwise, return to step 2. (1)



# Problem

The golden ratio method is the most economical analogue of the dichotomy method as applied to minimum tasks. This method is often used in technical or economic problems of optimization, when the minimized function is non-differentiable, and each calculation of the function is an expensive experiment.(10)
The golden ratio method is designed for deterministic tasks. In stochastic problems, due to experimental errors, one can incorrectly determine the relations between the values of functions at points; then further iterations can go the wrong way.(4)



# Solutions

We will test on this function.
$$
f(x) = x(x-2)(x+2)^2
$$

Let’s take a closer look at this algorithm and analyze it with code example.

First, let's add the necessary libraries

```
import numpy as np
import matplotlib.pyplot as plt
import seaborn
```

Next, we introduce the known data. 

```
# Target unimodal function on given segment
f = lambda x: (x - 2) * x * (x + 2)**2
x_true = -2
a = -1
b = 2
epsilon = 0.01
```

We minimize the function in accordance with the algorithm described in the chapter Formalization.

```
def golden_search(f, a, b, tol=1e-5, callback=None):
  tau = (np.sqrt(5) + 1) / 2.0
  y = a + (b - a) / tau**2
  z = a + (b - a) / tau
  while b - a > tol:
​    if f(y) <= f(z):
​      b = z
​      z = y
​      y = a + (b - a) / tau**2
​    else:
​      a = y
​      y = z
​      z = a + (b - a) / tau
​    if callback is not None:
​      callback(a, b)
  return (a + b) / 2.0
```



# Results

Выведем на экран искомые значения.

-9.914921122485794 

-9.914944669182807 

3.2822265625

![загруженное (1)](C:\Users\Astemir\Desktop\загруженное (1).png)

![загруженное](C:\Users\Astemir\Desktop\загруженное.png)

# Materials

*1.*Джон Г.Мэтьюз, Куртис Д.Финк.* "Численные методы. Использование MATLAB". — М, СПб: "Вильямс", 2001. — 716 с.

*2.*Alexei White. Major JavaScript     Engines // JavaScript Programmer's Reference. – Indianapolis, IN 46256: Wiley Publishing, Inc.,     2009. – P. 12—13. – (Programmer's Reference).

*3.*Максимов Ю. А., Филлиповская Е. А. Алгоритмы     решения задач нелинейного программирования. – М.: МИФИ, 1982.

*4.*http://scask.ru/q_book_dig_m.php?id=92

*5.*Пшеничный Б.Н.](https://ru.wikipedia.org/wiki/Пшеничный,_Борис_Николаевич_(математик)) Необходимые     условия экстремума. – М.: Наука, 1969. – 150 с.

*6*.Растригин Л. А. Статистические методы     поиска. – М., 1968.

*7.*https://en.wikipedia.org/wiki/Golden-section_search

*8*http://apps.nrbook.com/empanel/index.html#pg=1

*9*https://opt.mipt.ru/lectures/html/11%20Numerical%20Methods%20and%20Line%20Search.html

*10*https://www.studmed.ru/press-wh-teukolsky-sa-vetterling-wt-flannery-bp-numerical-recipes-in-c-the-art-of-scientific-computing_a0222f9890a.html

**Thanks for reading 😉**

Full work code and comparison of algorithms you can find below:

[![Open In Colab](https://camo.githubusercontent.com/e3c69ca1b09d36bcce13a6dd37a45fb86bbf0373/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e73766723627574746f6e)](https://colab.research.google.com/drive/18XbzoaLi5hqBHGXnVLyYFcRUVIcwrL8E#scrollTo=It3PEH1eZY1M&line=16&uniqifier=1)

 

